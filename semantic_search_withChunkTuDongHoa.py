import streamlit as st
import os
import docx
import fitz  
import pandas as pd  

import win32com.client
import tempfile

from underthesea import sent_tokenize
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("VoVanPhuc/sup-SimCSE-VietNamese-phobert-base")

import re

import faiss


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_DIR = os.path.join(BASE_DIR, "faiss_index")
os.makedirs(INDEX_DIR, exist_ok=True)

INDEX_PATH = os.path.join(INDEX_DIR, "index.faiss")
META_PATH = os.path.join(INDEX_DIR, "chunks.pkl")


def highlight_keywords(text, query):
    keywords = re.findall(r"\w+", query.lower())
    for kw in set(keywords):
        text = re.sub(fr"(?i)\b({re.escape(kw)})\b", r"<mark>\1</mark>", text)
    return text


def build_and_save_faiss_index(chunks, model, index_path, meta_path):
    texts = [c['text'] for c in chunks]
    doc_embs = model.encode(texts, show_progress_bar=True)
    faiss.normalize_L2(doc_embs)

    index = faiss.IndexFlatIP(doc_embs.shape[1])
    index.add(doc_embs.astype('float32'))

    faiss.write_index(index, index_path)
    with open(meta_path, "wb") as f:
        import pickle
        pickle.dump(chunks, f)
        

def load_faiss_index(index_path, meta_path):
    index = faiss.read_index(index_path)
    with open(meta_path, "rb") as f:
        import pickle
        chunks = pickle.load(f)
    return index, chunks



def preprocess_text(text):
    # 1. X·ª≠ l√Ω k√Ω t·ª± ƒëi·ªÅu khi·ªÉn th∆∞·ªùng g·∫∑p
    text = text.replace("\x0c", " ").replace("\xa0", " ")

    # 2. Lo·∫°i b·ªè d√≤ng tr·∫Øng v√† gom c√°c d√≤ng ng·∫Øt c√¢u sai c√°ch (OCR)
    lines = text.splitlines()
    clean_lines = [line.strip() for line in lines if line.strip()]
    merged_lines = []
    buffer = ""
    for line in clean_lines:
        if buffer:
            buffer += " " + line
        else:
            buffer = line

        if re.search(r"[.!?]$", line.strip()):
            merged_lines.append(buffer.strip())
            buffer = ""
    if buffer:
        merged_lines.append(buffer.strip())
    text = "\n\n".join(merged_lines)

    # 3. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng, k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = text.lower()
    text = re.sub(r"[^\w\s.,:%‚Äì\-‚Äì‚Äî]", "", text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\n+", "\n", text)

    return text.strip()

def convert_doc_to_docx(doc_path):
    if doc_path.endswith(".doc"):
        doc_path = os.path.abspath(doc_path)
        doc_path = os.path.normpath(doc_path)

        if not os.path.exists(doc_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {doc_path}")

        word = win32com.client.Dispatch("Word.Application")
        word.Visible = False

        # T·∫°o th∆∞ m·ª•c t·∫°m ƒë·ªÉ ch·ª©a .docx
        temp_dir = tempfile.mkdtemp()
        temp_docx_path = os.path.join(temp_dir, os.path.basename(doc_path) + "x")  # file.docx

        try:
            doc = word.Documents.Open(doc_path)
            doc.SaveAs(temp_docx_path, FileFormat=16)
            doc.Close()
            return temp_docx_path
        finally:
            word.Quit()
    return doc_path


# ---- ƒê·ªçc n·ªôi dung .docx ----
def read_docx(file_path):
    doc = docx.Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

# ---- ƒê·ªçc n·ªôi dung .pdf ----
def read_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# ---- ƒê·ªçc n·ªôi dung .xlsx v√† .xls ----
def read_excel(file_path):
    if file_path.endswith(".xlsx"):
        df = pd.read_excel(file_path, engine="openpyxl")
    elif file_path.endswith(".xls"):
        df = pd.read_excel(file_path, engine="xlrd")
    else:
        return ""
    text = df.astype(str).apply(lambda row: " ".join(row), axis=1).str.cat(sep="\n")
    return text


# def chunk_by_token_limit(text, tokenizer, max_tokens):
#     sentences = sent_tokenize(text)
#     chunks = []
#     current_chunk = ""
#     current_tokens = 0

#     for sent in sentences:
#         sent_tokens = len(tokenizer.encode(sent, add_special_tokens=False))
#         if current_tokens + sent_tokens <= max_tokens:
#             current_chunk += " " + sent
#             current_tokens += sent_tokens
#         else:
#             chunks.append(current_chunk.strip())
#             current_chunk = sent
#             current_tokens = sent_tokens

#     if current_chunk:
#         chunks.append(current_chunk.strip())

#     return chunks

# def auto_chunk(text, tokenizer, max_tokens):
#     return chunk_by_token_limit(text, tokenizer, max_tokens)


# ---- Chunk t·ª± ƒë·ªông theo n·ªôi dung ----
def auto_chunk(text, max_len=512):
    paragraphs = text.split("\n\n")
    chunks = []
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        num_words = len(para.split())
        num_sentences = len(sent_tokenize(para))

        if num_words < 150 or len(para) < max_len:
            chunks.append(para)  # ƒêo·∫°n ng·∫Øn, gi·ªØ nguy√™n
        elif num_sentences < 4:
            chunks += chunk_by_sliding_window(para, window_size=max_len, stride=max_len // 2)  # √çt c√¢u, d√†i ‚Üí sliding
        else:
            chunks += chunk_by_sentence(para, max_len)  # Nhi·ªÅu c√¢u ‚Üí chia theo c√¢u
    return chunks
    

def chunk_by_sentence(text, max_len=512):
    sentences = sent_tokenize(text)
    chunks = []
    current = ""
    for s in sentences:
        if len(current) + len(s) < max_len:
            current += " " + s
        else:
            chunks.append(current.strip())
            current = s
    if current:
        chunks.append(current.strip())
    return chunks

def chunk_by_sliding_window(text, window_size, stride):
    words = text.split()
    chunks = []
    for i in range(0, len(words), stride):
        chunk = " ".join(words[i:i + window_size])
        if len(chunk.strip()) > 0:
            chunks.append(chunk)
        if i + window_size >= len(words):
            break
    return chunks


# ---- Load & chunk t·∫•t c·∫£ vƒÉn b·∫£n ----
@st.cache_data
# def load_chunked_documents(folder, _tokenizer, max_tokens):
#     chunks = []
#     for filename in os.listdir(folder):
#         if filename.endswith((".docx", ".pdf", ".xlsx", ".doc", ".xls")):
#             file_path = os.path.join(folder, filename)

#             if filename.endswith(".doc"):
#                 file_path = convert_doc_to_docx(file_path)

#             if filename.endswith(".docx"):
#                 # raw_text = read_docx(file_path)
#                 raw_text = preprocess_text(read_docx(file_path))
#             elif filename.endswith(".pdf"):
#                 # raw_text = read_pdf(file_path)
#                 raw_text = preprocess_text(read_pdf(file_path))
#             elif filename.endswith((".xlsx", ".xls")):
#                 # raw_text = read_excel(file_path)
#                 raw_text = preprocess_text(read_excel(file_path))
#             else:
#                 continue
#             doc_chunks = auto_chunk(raw_text, _tokenizer, max_tokens)
#             for i, chunk in enumerate(doc_chunks):
#                 chunks.append({
#                     "filename": filename,
#                     "chunk_id": i,
#                     "text": chunk,
#                     "fulltext": raw_text
#                 })
#     return chunks

def load_chunked_documents(folder, max_len=512):
    chunks = []
    for filename in os.listdir(folder):
        if filename.endswith((".docx", ".pdf", ".xlsx", ".doc", ".xls")):
            file_path = os.path.join(folder, filename)

            if filename.endswith(".doc"):
                file_path = convert_doc_to_docx(file_path)

            if filename.endswith(".docx"):
                raw_text = read_docx(file_path)
                # raw_text = preprocess_text(read_docx(file_path))
            elif filename.endswith(".pdf"):
                raw_text = read_pdf(file_path)
                # raw_text = preprocess_text(read_pdf(file_path))
            elif filename.endswith((".xlsx", ".xls")):
                raw_text = read_excel(file_path)
                # raw_text = preprocess_text(read_excel(file_path))
            else:
                continue
            doc_chunks = auto_chunk(raw_text, max_len)
            for i, chunk in enumerate(doc_chunks):
                chunks.append({
                    "filename": filename,
                    "chunk_id": i,
                    "text": chunk,
                    "fulltext": raw_text
                })
    return chunks

# ---- T√¨m ki·∫øm ng·ªØ nghƒ©a ----
# def search_semantic_chunks(query, chunks, model, top_k=5):
#     query_emb = model.encode([query])
#     texts = [c['text'] for c in chunks]
#     doc_embs = model.encode(texts)
#     sims = cosine_similarity(query_emb, doc_embs).flatten()
    
#     # G·∫Øn ƒëi·ªÉm similarity v√†o t·ª´ng chunk
#     for i, sim in enumerate(sims):
#         chunks[i]['score'] = sim

#     # Nh√≥m theo filename: ch·ªâ gi·ªØ chunk c√≥ score cao nh·∫•t trong m·ªói file
#     best_chunks = {}
#     for c in chunks:
#         fname = c['filename']
#         if fname not in best_chunks or c['score'] > best_chunks[fname]['score']:
#             best_chunks[fname] = c

#     # S·∫Øp x·∫øp theo ƒëi·ªÉm cao nh·∫•t v√† l·∫•y top_k file duy nh·∫•t
#     sorted_chunks = sorted(best_chunks.values(), key=lambda x: x['score'], reverse=True)[:top_k]

#     # ƒê·ªãnh d·∫°ng l·∫°i k·∫øt qu·∫£ ƒë·∫ßu ra ƒë√∫ng theo format ban ƒë·∫ßu
#     results = []
#     for c in sorted_chunks:
#         results.append({
#             "filename": c["filename"],
#             "chunk_id": c["chunk_id"],
#             "score": c["score"],
#             "excerpt": c["text"],
#             "fulltext": c["fulltext"]
#         })
#     return results


def search_semantic_chunks(query, chunks, model, top_k=5):
    query_emb = model.encode([query])
    texts = [c['text'] for c in chunks]
    doc_embs = model.encode(texts)

    # FAISS expects float32
    index = faiss.IndexFlatIP(doc_embs.shape[1])  # Inner Product for cosine sim if vectors are normalized
    faiss.normalize_L2(doc_embs)
    faiss.normalize_L2(query_emb)
    index.add(doc_embs.astype("float32"))

    scores, indices = index.search(query_emb.astype("float32"), len(chunks))

    # G·∫Øn ƒëi·ªÉm similarity v√†o t·ª´ng chunk
    for i, idx in enumerate(indices[0]):
        chunks[idx]['score'] = float(scores[0][i])

    # Nh√≥m theo filename: ch·ªâ gi·ªØ chunk c√≥ score cao nh·∫•t trong m·ªói file
    best_chunks = {}
    for c in chunks:
        fname = c['filename']
        if fname not in best_chunks or c['score'] > best_chunks[fname]['score']:
            best_chunks[fname] = c

    # L·∫•y top_k file c√≥ ƒëi·ªÉm cao nh·∫•t
    sorted_chunks = sorted(best_chunks.values(), key=lambda x: x['score'], reverse=True)[:top_k]

    results = []
    for c in sorted_chunks:
        results.append({
            "filename": c["filename"],
            "chunk_id": c["chunk_id"],
            "score": c["score"],
            "excerpt": c["text"],
            "fulltext": c["fulltext"]
        })
    return results


def search_faiss_index(query, model, index, chunks, top_k=5):
    query_emb = model.encode([query])
    faiss.normalize_L2(query_emb)

    scores, indices = index.search(query_emb.astype("float32"), len(chunks))

    # G·∫Øn ƒëi·ªÉm similarity v√†o t·ª´ng chunk
    for i, idx in enumerate(indices[0]):
        chunks[idx]['score'] = float(scores[0][i])

    # L·ªçc top chunk m·ªói file
    best_chunks = {}
    for c in chunks:
        fname = c['filename']
        if fname not in best_chunks or c['score'] > best_chunks[fname]['score']:
            best_chunks[fname] = c

    sorted_chunks = sorted(best_chunks.values(), key=lambda x: x['score'], reverse=True)[:top_k]

    results = []
    for c in sorted_chunks:
        results.append({
            "filename": c["filename"],
            "chunk_id": c["chunk_id"],
            "score": c["score"],
            "excerpt": c["text"],
            "fulltext": c["fulltext"]
        })
    return results


# ==== Giao di·ªán Streamlit ====
st.set_page_config(page_title="T√¨m ki·∫øm ng·ªØ nghƒ©a", layout="wide")
st.title("T√¨m ki·∫øm vƒÉn b·∫£n theo ng·ªØ nghƒ©a (Chunk-based + PhoBERT)")
st.write("Nh·∫≠p m√¥ t·∫£ n·ªôi dung ho·∫∑c √Ω ƒë·ªãnh c·ªßa b·∫°n, h·ªá th·ªëng s·∫Ω t√¨m c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ n·ªôi dung li√™n quan.")

query = st.text_input("Nh·∫≠p n·ªôi dung t√¨m ki·∫øm", placeholder="v√≠ d·ª•: ph·∫ßn m·ªÅm qu·∫£n l√Ω thanh tra, x·ª≠ l√Ω r√°c th·∫£i,...")
submit = st.button("T√¨m ki·∫øm")

if submit and query:
    with st.spinner("ƒêang t·∫£i m√¥ h√¨nh v√† t√¨m ki·∫øm..."):
        model = SentenceTransformer("VoVanPhuc/sup-SimCSE-VietNamese-phobert-base")


        DATA_DIR = "File_Mau"
        # chunks = load_chunked_documents(DATA_DIR, max_len=512)

        if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):
            chunks = load_chunked_documents(DATA_DIR, max_len=512)
            build_and_save_faiss_index(chunks, model, INDEX_PATH, META_PATH)
            
        # Load index v√† metadata
        index, chunks = load_faiss_index(INDEX_PATH, META_PATH)

        
        # tokenizer = AutoTokenizer.from_pretrained("VoVanPhuc/sup-SimCSE-VietNamese-phobert-base")
        # max_len = tokenizer.model_max_length
        # chunks = load_chunked_documents(DATA_DIR, _tokenizer=tokenizer, max_tokens=max_len)

        # results = search_semantic_chunks(query, chunks, model)

        # T√¨m ki·∫øm t·ª´ index ƒë√£ build
        results = search_faiss_index(query, model, index, chunks)

        if results:
            st.success(f"T√¨m th·∫•y {len(results)} ƒëo·∫°n vƒÉn b·∫£n ph√π h·ª£p:")
            for res in results:
                st.markdown(f"### {res['filename']} ‚Äî Chunk {res['chunk_id']} ‚Äî Score: `{res['score']:.4f}`")
                # st.markdown(f"**Tr√≠ch ƒëo·∫°n:**\n\n{res['excerpt']}...")

                excerpt = res['excerpt']
                short_excerpt = excerpt[:500] + "..." if len(excerpt) > 500 else excerpt
                highlighted_excerpt = highlight_keywords(short_excerpt, query)
                # Hi·ªÉn th·ªã ƒëo·∫°n r√∫t g·ªçn ƒë√£ b√¥i s√°ng
                st.markdown(f"**Tr√≠ch ƒëo·∫°n:**<br>{highlighted_excerpt}", unsafe_allow_html=True)


                # with st.expander("Xem to√†n b·ªô n·ªôi dung g·ªëc c·ªßa vƒÉn b·∫£n"):
                #     # st.text(res["fulltext"])
                #     st.text_area("N·ªôi dung ƒë·∫ßy ƒë·ªß", res["fulltext"], height=300)
                # # st.markdown("---")

                
                # N√∫t t·∫£i file g·ªëc n·∫øu c√≥
                filepath = os.path.join(DATA_DIR, res['filename'])
                if os.path.exists(filepath):
                    with open(filepath, "rb") as f:
                        st.download_button("üìé T·∫£i file g·ªëc", f, file_name=res['filename'])


        else:
            st.warning("Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n ph√π h·ª£p.")
